#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
512690åˆ†å‹å’Œç¬”åˆ’åˆ†éªŒè¯è„šæœ¬

åŸºäºç¼ è®ºç†è®ºï¼ŒéªŒè¯512690çš„åˆ†å‹ï¼ˆé¡¶åˆ†å‹ã€åº•åˆ†å‹ï¼‰å’Œç¬”åˆ’åˆ†çš„æ­£ç¡®æ€§ã€‚

ä½œè€…: TradeTianYuan
æ—¥æœŸ: 2025-11-28
"""

import os
import json
import logging
import argparse
import numpy as np
import pandas as pd
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Union, Tuple

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger('512690ChanlunVerifier')

class ChanlunVerifier:
    """
    ç¼ è®ºéªŒè¯å™¨ç±»ï¼Œç”¨äºæ£€æµ‹åˆ†å‹å’Œç¬”åˆ’åˆ†
    """
    
    def __init__(self, symbol: str = '512690', data_dir: str = './data/daily'):
        """
        åˆå§‹åŒ–ç¼ è®ºéªŒè¯å™¨
        
        Args:
            symbol: è‚¡ç¥¨ä»£ç 
            data_dir: æ•°æ®ç›®å½•
        """
        self.symbol = symbol
        self.data_dir = data_dir
        self.daily_data = None
        self.fx_points = []  # åˆ†å‹ç‚¹åˆ—è¡¨
        self.bi_segments = []  # ç¬”åˆ’åˆ†åˆ—è¡¨
        
        # åˆ›å»ºç»“æœç›®å½•
        self.results_dir = './results'
        os.makedirs(self.results_dir, exist_ok=True)
    
    def load_data(self) -> bool:
        """
        åŠ è½½æ—¥çº¿æ•°æ®
        
        Returns:
            bool: æ˜¯å¦åŠ è½½æˆåŠŸ
        """
        try:
            data_file = os.path.join(self.data_dir, f'{self.symbol}_daily.csv')
            if not os.path.exists(data_file):
                logger.error(f"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_file}")
                return False
            
            self.daily_data = pd.read_csv(data_file)
            self.daily_data['date'] = pd.to_datetime(self.daily_data['date'])
            self.daily_data.sort_values('date', inplace=True)
            
            logger.info(f"æˆåŠŸåŠ è½½{self.symbol}æ—¥çº¿æ•°æ®ï¼Œå…±{len(self.daily_data)}æ¡è®°å½•")
            return True
        except Exception as e:
            logger.error(f"åŠ è½½æ•°æ®å¤±è´¥: {str(e)}")
            return False
    
    def detect_fx_points(self, lookback: int = 2) -> List[Dict]:
        """
        æ£€æµ‹åˆ†å‹ç‚¹ï¼ˆé¡¶åˆ†å‹å’Œåº•åˆ†å‹ï¼‰
        
        Args:
            lookback: å›çœ‹Kçº¿æ•°é‡
            
        Returns:
            List[Dict]: åˆ†å‹ç‚¹åˆ—è¡¨
        """
        if self.daily_data is None:
            logger.error("è¯·å…ˆåŠ è½½æ•°æ®")
            return []
        
        fx_points = []
        
        for i in range(lookback, len(self.daily_data) - lookback):
            # è·å–å½“å‰Kçº¿å’Œå‰åKçº¿
            current = self.daily_data.iloc[i]
            left_bars = [self.daily_data.iloc[i-j] for j in range(1, lookback+1)]
            right_bars = [self.daily_data.iloc[i+j] for j in range(1, lookback+1)]
            
            # æ£€æŸ¥é¡¶åˆ†å‹
            if (current['high'] > max([bar['high'] for bar in left_bars]) and 
                current['high'] > max([bar['high'] for bar in right_bars])):
                fx_points.append({
                    'date': current['date'],
                    'date_str': current['date'].strftime('%Y-%m-%d'),
                    'type': 'é¡¶åˆ†å‹',
                    'price': current['high'],
                    'index': i
                })
            
            # æ£€æŸ¥åº•åˆ†å‹
            elif (current['low'] < min([bar['low'] for bar in left_bars]) and 
                  current['low'] < min([bar['low'] for bar in right_bars])):
                fx_points.append({
                    'date': current['date'],
                    'date_str': current['date'].strftime('%Y-%m-%d'),
                    'type': 'åº•åˆ†å‹',
                    'price': current['low'],
                    'index': i
                })
        
        # è¿‡æ»¤æ‰é‡åˆçš„åˆ†å‹
        filtered_fx = []
        for fx in fx_points:
            keep = True
            for i, existing in enumerate(filtered_fx):
                if existing['type'] == fx['type']:
                    # åŒä¸€ç±»å‹çš„åˆ†å‹ï¼Œå¦‚æœæ–°çš„ä»·æ ¼æ›´é«˜ï¼ˆé¡¶åˆ†å‹ï¼‰æˆ–æ›´ä½ï¼ˆåº•åˆ†å‹ï¼‰ï¼Œåˆ™æ›¿æ¢
                    if (existing['type'] == 'é¡¶åˆ†å‹' and fx['price'] > existing['price']) or \
                       (existing['type'] == 'åº•åˆ†å‹' and fx['price'] < existing['price']):
                        filtered_fx[i] = fx
                        keep = False
                    else:
                        keep = False
            if keep:
                filtered_fx.append(fx)
        
        # æŒ‰æ—¶é—´æ’åº
        filtered_fx.sort(key=lambda x: x['date'])
        
        self.fx_points = filtered_fx
        logger.info(f"æ£€æµ‹åˆ°{len(filtered_fx)}ä¸ªåˆ†å‹ç‚¹ï¼ˆ{sum(1 for fx in filtered_fx if fx['type'] == 'é¡¶åˆ†å‹')}ä¸ªé¡¶åˆ†å‹ï¼Œ{sum(1 for fx in filtered_fx if fx['type'] == 'åº•åˆ†å‹')}ä¸ªåº•åˆ†å‹ï¼‰")
        return filtered_fx
    
    def divide_bi_segments(self) -> List[Dict]:
        """
        åˆ’åˆ†ç¬”
        æ³¨ï¼šç®€åŒ–ç‰ˆæœ¬ï¼Œå®é™…ç¼ è®ºçš„ç¬”åˆ’åˆ†éœ€è¦è€ƒè™‘åŒ…å«å…³ç³»å¤„ç†ã€åˆ†å‹ç¡®è®¤ç­‰å¤æ‚è§„åˆ™
        
        Returns:
            List[Dict]: ç¬”åˆ’åˆ†åˆ—è¡¨
        """
        if not self.fx_points:
            logger.error("è¯·å…ˆæ£€æµ‹åˆ†å‹ç‚¹")
            return []
        
        bi_segments = []
        current_fx = None
        
        for fx in self.fx_points:
            if current_fx is None:
                current_fx = fx
                continue
            
            # ç¡®ä¿åˆ†å‹ç±»å‹äº¤æ›¿
            if current_fx['type'] != fx['type']:
                # è®¡ç®—ç¬”çš„æ–¹å‘å’Œé•¿åº¦
                direction = 'å‘ä¸Šç¬”' if current_fx['type'] == 'åº•åˆ†å‹' and fx['type'] == 'é¡¶åˆ†å‹' else 'å‘ä¸‹ç¬”'
                price_change = fx['price'] - current_fx['price']
                percent_change = (price_change / current_fx['price']) * 100
                
                bi_segments.append({
                    'start_date': current_fx['date_str'],
                    'end_date': fx['date_str'],
                    'start_price': current_fx['price'],
                    'end_price': fx['price'],
                    'direction': direction,
                    'price_change': price_change,
                    'percent_change': percent_change,
                    'start_type': current_fx['type'],
                    'end_type': fx['type'],
                    'bar_count': fx['index'] - current_fx['index'] + 1
                })
                
                current_fx = fx
        
        # æ·»åŠ æœ€åä¸€ä¸ªå¯èƒ½æœªå®Œæˆçš„ç¬”
        if len(bi_segments) > 0:
            last_bi = bi_segments[-1]
            last_fx = self.fx_points[-1]
            latest_bar = self.daily_data.iloc[-1]
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æ–°çš„ç¬”å½¢æˆçš„å¯èƒ½
            if last_bi['end_type'] != 'åº•åˆ†å‹' and latest_bar['low'] < last_fx['price']:
                bi_segments.append({
                    'start_date': last_fx['date_str'],
                    'end_date': latest_bar['date'].strftime('%Y-%m-%d'),
                    'start_price': last_fx['price'],
                    'end_price': latest_bar['low'],
                    'direction': 'å‘ä¸‹ç¬”',
                    'price_change': latest_bar['low'] - last_fx['price'],
                    'percent_change': ((latest_bar['low'] / last_fx['price']) - 1) * 100,
                    'start_type': last_fx['type'],
                    'end_type': 'æ½œåœ¨åº•åˆ†å‹',
                    'bar_count': len(self.daily_data) - last_fx['index']
                })
            elif last_bi['end_type'] != 'é¡¶åˆ†å‹' and latest_bar['high'] > last_fx['price']:
                bi_segments.append({
                    'start_date': last_fx['date_str'],
                    'end_date': latest_bar['date'].strftime('%Y-%m-%d'),
                    'start_price': last_fx['price'],
                    'end_price': latest_bar['high'],
                    'direction': 'å‘ä¸Šç¬”',
                    'price_change': latest_bar['high'] - last_fx['price'],
                    'percent_change': ((latest_bar['high'] / last_fx['price']) - 1) * 100,
                    'start_type': last_fx['type'],
                    'end_type': 'æ½œåœ¨é¡¶åˆ†å‹',
                    'bar_count': len(self.daily_data) - last_fx['index']
                })
        
        self.bi_segments = bi_segments
        logger.info(f"åˆ’åˆ†å‡º{len(bi_segments)}ä¸ªç¬”")
        return bi_segments
    
    def verify_fx_quality(self) -> Dict:
        """
        éªŒè¯åˆ†å‹è´¨é‡
        
        Returns:
            Dict: åˆ†å‹è´¨é‡åˆ†æç»“æœ
        """
        if not self.fx_points:
            return {
                'total_fx': 0,
                'top_fx': 0,
                'bottom_fx': 0,
                'avg_distance': 0,
                'quality_stats': {}
            }
        
        # è®¡ç®—åˆ†å‹é—´è·ç»Ÿè®¡
        distances = []
        for i in range(1, len(self.fx_points)):
            distance = (self.fx_points[i]['date'] - self.fx_points[i-1]['date']).days
            distances.append(distance)
        
        # è®¡ç®—åˆ†å‹å¯é æ€§æŒ‡æ ‡ï¼ˆç®€åŒ–ç‰ˆï¼‰
        reliable_fx = []
        for fx in self.fx_points:
            # æŸ¥æ‰¾é™„è¿‘çš„Kçº¿
            idx = fx['index']
            nearby_bars = self.daily_data.iloc[max(0, idx-5):min(len(self.daily_data), idx+5)]
            
            # è®¡ç®—æŒ¯å¹…
            amplitude = (nearby_bars['high'].max() - nearby_bars['low'].min()) / nearby_bars['close'].mean() * 100
            
            # è®¡ç®—æˆäº¤é‡å˜åŒ–
            vol_change = 0
            if idx > 0:
                prev_vol = self.daily_data.iloc[idx-1]['volume']
                curr_vol = self.daily_data.iloc[idx]['volume']
                vol_change = (curr_vol / prev_vol - 1) * 100
            
            # ç®€å•çš„å¯é æ€§è¯„åˆ†
            reliability = 0
            if amplitude > 2.0:  # æŒ¯å¹…å¤§äº2%è®¤ä¸ºæ¯”è¾ƒå¯é 
                reliability += 50
            if abs(vol_change) > 30:  # æˆäº¤é‡å˜åŒ–å¤§äº30%è®¤ä¸ºæœ‰ä¸€å®šå¯é æ€§
                reliability += 30
            if abs(fx['price'] - self.daily_data.iloc[idx]['close']) / fx['price'] < 0.01:  # åˆ†å‹ç‚¹æ¥è¿‘æ”¶ç›˜ä»·
                reliability += 20
            
            reliable_fx.append({
                'date_str': fx['date_str'],
                'type': fx['type'],
                'price': fx['price'],
                'amplitude': amplitude,
                'volume_change': vol_change,
                'reliability_score': reliability
            })
        
        high_quality = sum(1 for fx in reliable_fx if fx['reliability_score'] > 70)
        medium_quality = sum(1 for fx in reliable_fx if 40 <= fx['reliability_score'] <= 70)
        low_quality = sum(1 for fx in reliable_fx if fx['reliability_score'] < 40)
        
        return {
            'total_fx': len(self.fx_points),
            'top_fx': sum(1 for fx in self.fx_points if fx['type'] == 'é¡¶åˆ†å‹'),
            'bottom_fx': sum(1 for fx in self.fx_points if fx['type'] == 'åº•åˆ†å‹'),
            'avg_distance': np.mean(distances) if distances else 0,
            'quality_stats': {
                'high_quality': high_quality,
                'medium_quality': medium_quality,
                'low_quality': low_quality,
                'avg_reliability': np.mean([fx['reliability_score'] for fx in reliable_fx]) if reliable_fx else 0
            },
            'detailed_fx': reliable_fx
        }
    
    def verify_bi_correctness(self) -> Dict:
        """
        éªŒè¯ç¬”åˆ’åˆ†æ­£ç¡®æ€§
        
        Returns:
            Dict: ç¬”åˆ’åˆ†æ­£ç¡®æ€§åˆ†æç»“æœ
        """
        if not self.bi_segments:
            return {
                'total_bi': 0,
                'up_bi': 0,
                'down_bi': 0,
                'avg_length': 0,
                'correctness_stats': {}
            }
        
        # è®¡ç®—ç¬”é•¿åº¦ç»Ÿè®¡
        up_bi_lengths = [bi['percent_change'] for bi in self.bi_segments if bi['direction'] == 'å‘ä¸Šç¬”']
        down_bi_lengths = [bi['percent_change'] for bi in self.bi_segments if bi['direction'] == 'å‘ä¸‹ç¬”']
        avg_up_length = np.mean(up_bi_lengths) if up_bi_lengths else 0
        avg_down_length = np.mean(down_bi_lengths) if down_bi_lengths else 0
        
        # éªŒè¯ç¬”çš„å»¶ç»­æ€§
        continuity_score = 0
        standard_bi_count = 0  # ç¬¦åˆæ ‡å‡†çš„ç¬”æ•°é‡
        
        for bi in self.bi_segments:
            # æ£€æŸ¥ç¬”çš„é•¿åº¦æ˜¯å¦åˆç†ï¼ˆè‡³å°‘1%ï¼‰
            if abs(bi['percent_change']) >= 1.0:
                standard_bi_count += 1
                continuity_score += 30
            
            # æ£€æŸ¥Kçº¿æ•°é‡æ˜¯å¦åˆç†ï¼ˆè‡³å°‘5æ ¹ï¼‰
            if bi['bar_count'] >= 5:
                continuity_score += 30
            
            # æ£€æŸ¥æ˜¯å¦æœ‰é‡å ï¼ˆç®€åŒ–åˆ¤æ–­ï¼‰
            if len(self.bi_segments) > 1:
                continuity_score += 20
        
        continuity_score = continuity_score / len(self.bi_segments) if self.bi_segments else 0
        
        # æ£€æŸ¥æœ€è¿‘çš„ç¬”æ˜¯å¦å½¢æˆä¸­æ¢
        has_zhongshu = False
        if len(self.bi_segments) >= 3:
            # ç®€åŒ–çš„ä¸­æ¢åˆ¤æ–­ï¼šæœ€è¿‘3ç¬”æ˜¯å¦æœ‰ä»·æ ¼é‡å 
            recent_3bi = self.bi_segments[-3:]
            if len(recent_3bi) == 3:
                # è·å–ä»·æ ¼èŒƒå›´
                all_prices = []
                for bi in recent_3bi:
                    all_prices.extend([bi['start_price'], bi['end_price']])
                
                # æ£€æŸ¥æ˜¯å¦å½¢æˆé‡å åŒºé—´
                if max(min([bi['start_price'], bi['end_price']]) for bi in recent_3bi) < \
                   min(max([bi['start_price'], bi['end_price']]) for bi in recent_3bi):
                    has_zhongshu = True
        
        return {
            'total_bi': len(self.bi_segments),
            'up_bi': sum(1 for bi in self.bi_segments if bi['direction'] == 'å‘ä¸Šç¬”'),
            'down_bi': sum(1 for bi in self.bi_segments if bi['direction'] == 'å‘ä¸‹ç¬”'),
            'avg_length': np.mean([abs(bi['percent_change']) for bi in self.bi_segments]) if self.bi_segments else 0,
            'avg_up_length': avg_up_length,
            'avg_down_length': avg_down_length,
            'correctness_stats': {
                'standard_bi_count': standard_bi_count,
                'standard_bi_ratio': standard_bi_count / len(self.bi_segments) if self.bi_segments else 0,
                'continuity_score': continuity_score,
                'has_zhongshu': has_zhongshu
            }
        }
    
    def generate_verification_report(self) -> str:
        """
        ç”ŸæˆéªŒè¯æŠ¥å‘Š
        
        Returns:
            str: éªŒè¯æŠ¥å‘Šæ–‡æœ¬
        """
        # éªŒè¯åˆ†å‹è´¨é‡
        fx_quality = self.verify_fx_quality()
        
        # éªŒè¯ç¬”åˆ’åˆ†æ­£ç¡®æ€§
        bi_correctness = self.verify_bi_correctness()
        
        # ç”ŸæˆæŠ¥å‘Š
        report = []
        report.append(f"===== {self.symbol}åˆ†å‹å’Œç¬”åˆ’åˆ†éªŒè¯æŠ¥å‘Š =====")
        report.append(f"éªŒè¯æ—¥æœŸ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report.append(f"æ•°æ®èŒƒå›´: {self.daily_data.iloc[0]['date'].strftime('%Y-%m-%d')} è‡³ {self.daily_data.iloc[-1]['date'].strftime('%Y-%m-%d')}")
        report.append("")
        
        # åˆ†å‹åˆ†æ
        report.append("ğŸ“Š åˆ†å‹åˆ†æ:")
        report.append("-" * 80)
        report.append(f"åˆ†å‹æ€»æ•°: {fx_quality['total_fx']}ä¸ª")
        report.append(f"é¡¶åˆ†å‹: {fx_quality['top_fx']}ä¸ª")
        report.append(f"åº•åˆ†å‹: {fx_quality['bottom_fx']}ä¸ª")
        report.append(f"å¹³å‡åˆ†å‹é—´è·: {fx_quality['avg_distance']:.1f}å¤©")
        report.append("")
        
        # åˆ†å‹è´¨é‡ç»Ÿè®¡
        quality = fx_quality['quality_stats']
        report.append(f"åˆ†å‹è´¨é‡ç»Ÿè®¡:")
        report.append(f"  - é«˜è´¨é‡åˆ†å‹(>70åˆ†): {quality['high_quality']}ä¸ª ({quality['high_quality']/fx_quality['total_fx']*100:.1f}%)")
        report.append(f"  - ä¸­ç­‰è´¨é‡åˆ†å‹(40-70åˆ†): {quality['medium_quality']}ä¸ª ({quality['medium_quality']/fx_quality['total_fx']*100:.1f}%)")
        report.append(f"  - ä½è´¨é‡åˆ†å‹(<40åˆ†): {quality['low_quality']}ä¸ª ({quality['low_quality']/fx_quality['total_fx']*100:.1f}%)")
        report.append(f"  - å¹³å‡å¯é æ€§è¯„åˆ†: {quality['avg_reliability']:.1f}/100")
        report.append("")
        
        # æœ€è¿‘çš„åˆ†å‹
        recent_fx = sorted(fx_quality['detailed_fx'], key=lambda x: x['date_str'], reverse=True)[:10]
        report.append("æœ€è¿‘çš„åˆ†å‹ç‚¹:")
        report.append(f"{'æ—¥æœŸ':<15} {'ç±»å‹':<10} {'ä»·æ ¼':<10} {'å¯é æ€§è¯„åˆ†':<15} {'æŒ¯å¹…(%)':<10} {'æˆäº¤é‡å˜åŒ–(%)':<15}")
        report.append("-" * 80)
        for fx in recent_fx:
            report.append(f"{fx['date_str']:<15} {fx['type']:<10} {fx['price']:<10.3f} {fx['reliability_score']:<15.1f} {fx['amplitude']:<10.2f} {fx['volume_change']:<15.2f}")
        report.append("")
        
        # ç¬”åˆ’åˆ†åˆ†æ
        report.append("ğŸ“ˆ ç¬”åˆ’åˆ†åˆ†æ:")
        report.append("-" * 80)
        report.append(f"ç¬”æ€»æ•°: {bi_correctness['total_bi']}ä¸ª")
        report.append(f"å‘ä¸Šç¬”: {bi_correctness['up_bi']}ä¸ª")
        report.append(f"å‘ä¸‹ç¬”: {bi_correctness['down_bi']}ä¸ª")
        report.append(f"å¹³å‡ç¬”é•¿åº¦: {bi_correctness['avg_length']:.2f}%")
        report.append(f"å¹³å‡å‘ä¸Šç¬”é•¿åº¦: {bi_correctness['avg_up_length']:.2f}%")
        report.append(f"å¹³å‡å‘ä¸‹ç¬”é•¿åº¦: {bi_correctness['avg_down_length']:.2f}%")
        report.append("")
        
        # ç¬”åˆ’åˆ†æ­£ç¡®æ€§
        correctness = bi_correctness['correctness_stats']
        report.append(f"ç¬”åˆ’åˆ†æ­£ç¡®æ€§ç»Ÿè®¡:")
        report.append(f"  - æ ‡å‡†ç¬”æ•°é‡(é•¿åº¦â‰¥1%): {correctness['standard_bi_count']}ä¸ª ({correctness['standard_bi_ratio']*100:.1f}%)")
        report.append(f"  - å»¶ç»­æ€§è¯„åˆ†: {correctness['continuity_score']:.1f}/100")
        report.append(f"  - æ˜¯å¦å½¢æˆä¸­æ¢: {'æ˜¯' if correctness['has_zhongshu'] else 'å¦'}")
        report.append("")
        
        # æœ€è¿‘çš„ç¬”
        recent_bi = sorted(self.bi_segments, key=lambda x: x['end_date'], reverse=True)[:5]
        report.append("æœ€è¿‘çš„ç¬”åˆ’åˆ†:")
        report.append(f"{'èµ·å§‹æ—¥æœŸ':<15} {'ç»“æŸæ—¥æœŸ':<15} {'æ–¹å‘':<10} {'æ¶¨è·Œå¹…(%)':<12} {'Kçº¿æ•°é‡':<10} {'ç»“æŸç±»å‹':<15}")
        report.append("-" * 80)
        for bi in recent_bi:
            change_str = f"{bi['percent_change']:+.2f}%" if bi['percent_change'] != 0 else "0.00%"
            report.append(f"{bi['start_date']:<15} {bi['end_date']:<15} {bi['direction']:<10} {change_str:<12} {bi['bar_count']:<10} {bi['end_type']:<15}")
        report.append("")
        
        # éªŒè¯ç»“è®º
        report.append("ğŸ“ éªŒè¯ç»“è®º:")
        report.append("-" * 80)
        
        # åˆ†å‹éªŒè¯ç»“è®º
        if fx_quality['total_fx'] > 0:
            if quality['avg_reliability'] > 70:
                report.append("âœ… åˆ†å‹è´¨é‡è‰¯å¥½: åˆ†å‹æ¸…æ™°ï¼Œå¯é æ€§é«˜")
            elif quality['avg_reliability'] > 50:
                report.append("âš ï¸ åˆ†å‹è´¨é‡ä¸­ç­‰: éƒ¨åˆ†åˆ†å‹å¯é æ€§ä¸€èˆ¬ï¼Œå»ºè®®ç»“åˆå…¶ä»–æŒ‡æ ‡")
            else:
                report.append("âŒ åˆ†å‹è´¨é‡è¾ƒå·®: åˆ†å‹ä¸æ¸…æ™°ï¼Œå¯é æ€§ä½ï¼Œéœ€è¦è°¨æ…åˆ¤æ–­")
        else:
            report.append("âŒ æœªæ£€æµ‹åˆ°åˆ†å‹: å¯èƒ½æ•°æ®ä¸è¶³æˆ–å¸‚åœºæ³¢åŠ¨è¾ƒå°")
        
        # ç¬”åˆ’åˆ†éªŒè¯ç»“è®º
        if bi_correctness['total_bi'] > 0:
            if correctness['continuity_score'] > 70 and correctness['standard_bi_ratio'] > 0.8:
                report.append("âœ… ç¬”åˆ’åˆ†åˆç†: ç¬”çš„æ•°é‡å’Œè´¨é‡ç¬¦åˆç¼ è®ºè¦æ±‚")
            elif correctness['continuity_score'] > 50:
                report.append("âš ï¸ ç¬”åˆ’åˆ†åŸºæœ¬åˆç†: éƒ¨åˆ†ç¬”å¯èƒ½éœ€è¦è°ƒæ•´")
            else:
                report.append("âŒ ç¬”åˆ’åˆ†å­˜åœ¨é—®é¢˜: ç¬”çš„å»¶ç»­æ€§å·®ï¼Œå»ºè®®é‡æ–°åˆ’åˆ†")
            
            if correctness['has_zhongshu']:
                report.append("ğŸ”„ å·²å½¢æˆä¸­æ¢: å½“å‰èµ°åŠ¿å¤„äºä¸­æ¢éœ‡è¡é˜¶æ®µ")
        else:
            report.append("âŒ æœªå®Œæˆç¬”åˆ’åˆ†: å¯èƒ½åˆ†å‹æ•°é‡ä¸è¶³")
        
        # ç»¼åˆå»ºè®®
        report.append("")
        report.append("ğŸ¯ äº¤æ˜“å»ºè®®:")
        report.append("-" * 80)
        if len(self.bi_segments) > 0:
            last_bi = self.bi_segments[-1]
            if last_bi['direction'] == 'å‘ä¸Šç¬”':
                if last_bi['percent_change'] > 3:  # å‘ä¸Šç¬”å¹…åº¦è¾ƒå¤§
                    report.append("ğŸ“‰ æ³¨æ„é£é™©: å½“å‰å¤„äºå‘ä¸Šç¬”ä¸­ï¼Œæ¶¨å¹…è¾ƒå¤§ï¼Œå¯èƒ½æ¥è¿‘é¡¶éƒ¨")
                else:
                    report.append("ğŸ“ˆ å…³æ³¨ä¹°å…¥æœºä¼š: å½“å‰å¤„äºå‘ä¸Šç¬”åˆå§‹é˜¶æ®µ")
            else:
                if last_bi['percent_change'] < -3:  # å‘ä¸‹ç¬”å¹…åº¦è¾ƒå¤§
                    report.append("ğŸ“ˆ å…³æ³¨ä¹°å…¥æœºä¼š: å‘ä¸‹ç¬”å¹…åº¦è¾ƒå¤§ï¼Œå¯èƒ½æ¥è¿‘åº•éƒ¨")
                else:
                    report.append("ğŸ” ç»§ç»­è§‚å¯Ÿ: å½“å‰å¤„äºå‘ä¸‹ç¬”ä¸­ï¼Œå»ºè®®ç­‰å¾…æ˜ç¡®ä¿¡å·")
        else:
            report.append("ğŸ” æ•°æ®ä¸è¶³: å»ºè®®ç§¯ç´¯æ›´å¤šæ•°æ®åå†è¿›è¡Œåˆ¤æ–­")
        
        report.append("")
        report.append("âš ï¸ é£é™©æç¤º:")
        report.append("-" * 80)
        report.append("1. æœ¬éªŒè¯åŸºäºç®€åŒ–çš„ç¼ è®ºè§„åˆ™ï¼Œä»…ä¾›å‚è€ƒ")
        report.append("2. å®é™…æ“ä½œä¸­åº”ç»“åˆå…¶ä»–æŠ€æœ¯åˆ†ææ–¹æ³•")
        report.append("3. å¸‚åœºæœ‰é£é™©ï¼ŒæŠ•èµ„éœ€è°¨æ…")
        
        return "\n".join(report)
    
    def save_results(self, report: str) -> None:
        """
        ä¿å­˜éªŒè¯ç»“æœ
        
        Args:
            report: éªŒè¯æŠ¥å‘Šæ–‡æœ¬
        """
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # ä¿å­˜æŠ¥å‘Šæ–‡æœ¬
        report_file = os.path.join(self.results_dir, f'{self.symbol}_chanlun_verification_{timestamp}.txt')
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report)
        logger.info(f"éªŒè¯æŠ¥å‘Šå·²ä¿å­˜è‡³: {report_file}")
        
        # ä¿å­˜åˆ†å‹æ•°æ®
        fx_file = os.path.join(self.results_dir, f'{self.symbol}_fx_points_{timestamp}.json')
        # è½¬æ¢datetimeå¯¹è±¡ä¸ºå­—ç¬¦ä¸²ä»¥JSONåºåˆ—åŒ–
        serializable_fx = []
        for fx in self.fx_points:
            fx_copy = fx.copy()
            fx_copy['date'] = fx_copy['date'].strftime('%Y-%m-%d')
            serializable_fx.append(fx_copy)
        
        with open(fx_file, 'w', encoding='utf-8') as f:
            json.dump(serializable_fx, f, ensure_ascii=False, indent=2)
        logger.info(f"åˆ†å‹æ•°æ®å·²ä¿å­˜è‡³: {fx_file}")
        
        # ä¿å­˜ç¬”æ•°æ®
        bi_file = os.path.join(self.results_dir, f'{self.symbol}_bi_segments_{timestamp}.json')
        with open(bi_file, 'w', encoding='utf-8') as f:
            json.dump(self.bi_segments, f, ensure_ascii=False, indent=2)
        logger.info(f"ç¬”åˆ’åˆ†æ•°æ®å·²ä¿å­˜è‡³: {bi_file}")

def parse_args():
    """
    è§£æå‘½ä»¤è¡Œå‚æ•°
    """
    parser = argparse.ArgumentParser(description='512690åˆ†å‹å’Œç¬”åˆ’åˆ†éªŒè¯è„šæœ¬')
    parser.add_argument('--symbol', type=str, default='512690', help='è‚¡ç¥¨ä»£ç ')
    parser.add_argument('--data_dir', type=str, default='./data/daily', help='æ•°æ®ç›®å½•')
    parser.add_argument('--lookback', type=int, default=2, help='æ£€æµ‹åˆ†å‹çš„å›çœ‹Kçº¿æ•°é‡')
    return parser.parse_args()

def main():
    """
    ä¸»å‡½æ•°
    """
    args = parse_args()
    
    # åˆ›å»ºéªŒè¯å™¨å®ä¾‹
    verifier = ChanlunVerifier(symbol=args.symbol, data_dir=args.data_dir)
    
    # åŠ è½½æ•°æ®
    if not verifier.load_data():
        logger.error("åŠ è½½æ•°æ®å¤±è´¥ï¼Œé€€å‡ºç¨‹åº")
        return
    
    # æ£€æµ‹åˆ†å‹ç‚¹
    verifier.detect_fx_points(lookback=args.lookback)
    
    # åˆ’åˆ†ç¬”
    verifier.divide_bi_segments()
    
    # ç”ŸæˆéªŒè¯æŠ¥å‘Š
    report = verifier.generate_verification_report()
    print(report)
    
    # ä¿å­˜ç»“æœ
    verifier.save_results(report)

if __name__ == "__main__":
    main()